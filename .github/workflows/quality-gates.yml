name: Quality Gates

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run quality checks daily at 2 AM UTC
    - cron: '0 2 * * *'

env:
  NODE_VERSION: '20'
  CACHE_DEPENDENCY_PATH: '**/package-lock.json'

jobs:
  code-quality:
    name: Code Quality Analysis
    runs-on: ubuntu-latest
    
    outputs:
      quality_score: ${{ steps.quality-metrics.outputs.quality_score }}
      
    steps:
    - name: Checkout Code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for better analysis
      
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        cache-dependency-path: ${{ env.CACHE_DEPENDENCY_PATH }}
        
    - name: Install Dependencies
      run: npm ci
      
    - name: TypeScript Strict Checking
      run: |
        echo "🔍 Running TypeScript strict analysis..."
        
        # Check all packages with strict mode
        for pkg in packages/*/tsconfig.json; do
          if [ -f "$pkg" ]; then
            dir=$(dirname "$pkg")
            echo "Checking $dir..."
            cd "$dir"
            npx tsc --noEmit --strict
            cd ../..
          fi
        done
        
        # Check frontend
        if [ -f "frontend/tsconfig.json" ]; then
          echo "Checking frontend..."
          cd frontend
          npx tsc --noEmit --strict
          cd ..
        fi
        
        # Check functions
        if [ -f "functions/tsconfig.json" ]; then
          echo "Checking functions..."
          cd functions
          npx tsc --noEmit --strict
          cd ..
        fi
        
        echo "✅ TypeScript strict checking completed"
        
    - name: ESLint Analysis
      run: |
        echo "🔍 Running ESLint analysis..."
        
        # Run ESLint on all workspaces
        npm run lint 2>/dev/null || true
        
        # Generate ESLint report
        npx eslint packages/ frontend/src/ functions/src/ \
          --ext .ts,.tsx,.js,.jsx \
          --format json \
          --output-file eslint-report.json \
          --ignore-path .gitignore || true
        
        # Count issues
        ERRORS=$(cat eslint-report.json | jq '[.[].messages[] | select(.severity == 2)] | length')
        WARNINGS=$(cat eslint-report.json | jq '[.[].messages[] | select(.severity == 1)] | length')
        
        echo "ESLint Results:"
        echo "- Errors: $ERRORS"
        echo "- Warnings: $WARNINGS"
        
        # Fail if there are errors
        if [ "$ERRORS" -gt "0" ]; then
          echo "❌ ESLint errors must be fixed before merging"
          exit 1
        fi
        
        echo "✅ ESLint analysis completed"
        
    - name: Code Complexity Analysis
      run: |
        echo "🔍 Analyzing code complexity..."
        
        # Install complexity analysis tool
        npm install -g complexity-report
        
        # Analyze complexity for key files
        find packages/ frontend/src/ functions/src/ -name "*.ts" -o -name "*.tsx" | while read file; do
          if [ -f "$file" ]; then
            echo "Analyzing: $file"
            cr "$file" --format json > /dev/null 2>&1 || true
          fi
        done
        
        echo "✅ Complexity analysis completed"
        
    - name: Test Coverage Analysis
      run: |
        echo "🔍 Analyzing test coverage..."
        
        # Run tests with coverage for all packages
        npm run test 2>/dev/null || echo "Some tests failed"
        
        # Collect coverage data
        COVERAGE_DATA=""
        for pkg in packages/*/coverage/lcov.info; do
          if [ -f "$pkg" ]; then
            COVERAGE_DATA="$COVERAGE_DATA $pkg"
          fi
        done
        
        # Generate combined coverage report
        if [ -n "$COVERAGE_DATA" ]; then
          echo "Coverage files found: $COVERAGE_DATA"
        else
          echo "No coverage files found"
        fi
        
        echo "✅ Test coverage analysis completed"
        
    - name: Calculate Quality Metrics
      id: quality-metrics
      run: |
        echo "📊 Calculating quality metrics..."
        
        # Read ESLint results
        ERRORS=$(cat eslint-report.json | jq '[.[].messages[] | select(.severity == 2)] | length')
        WARNINGS=$(cat eslint-report.json | jq '[.[].messages[] | select(.severity == 1)] | length')
        
        # Calculate quality score (0-100)
        QUALITY_SCORE=100
        
        # Deduct points for errors and warnings
        QUALITY_SCORE=$((QUALITY_SCORE - ERRORS * 5))  # 5 points per error
        QUALITY_SCORE=$((QUALITY_SCORE - WARNINGS * 1))  # 1 point per warning
        
        # Ensure score doesn't go below 0
        if [ "$QUALITY_SCORE" -lt "0" ]; then
          QUALITY_SCORE=0
        fi
        
        echo "quality_score=$QUALITY_SCORE" >> $GITHUB_OUTPUT
        
        echo "Quality Score: $QUALITY_SCORE/100"
        echo "- ESLint Errors: $ERRORS"
        echo "- ESLint Warnings: $WARNINGS"
        
        # Quality gate threshold
        if [ "$QUALITY_SCORE" -lt "85" ]; then
          echo "❌ Quality score ($QUALITY_SCORE) below threshold (85)"
          exit 1
        else
          echo "✅ Quality score meets threshold"
        fi
        
    - name: Upload Quality Reports
      uses: actions/upload-artifact@v4
      with:
        name: quality-reports
        path: |
          eslint-report.json
        retention-days: 30

  security-analysis:
    name: Security Analysis
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout Code
      uses: actions/checkout@v4
      
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        cache-dependency-path: ${{ env.CACHE_DEPENDENCY_PATH }}
        
    - name: Install Dependencies
      run: npm ci
      
    - name: NPM Audit
      run: |
        echo "🔒 Running NPM security audit..."
        
        # Run audit for main project
        npm audit --audit-level high
        
        # Run audit for each workspace
        for workspace in packages/* frontend functions; do
          if [ -d "$workspace" ] && [ -f "$workspace/package.json" ]; then
            echo "Auditing $workspace..."
            cd "$workspace"
            npm audit --audit-level high || echo "Security issues found in $workspace"
            cd ..
          fi
        done
        
        echo "✅ NPM audit completed"
        
    - name: Secret Scanning
      run: |
        echo "🔍 Scanning for exposed secrets..."
        
        # Check for common secret patterns
        SECRET_PATTERNS=(
          "apiKey.*[\"'][A-Za-z0-9]{20,}[\"']"
          "api_key.*[\"'][A-Za-z0-9]{20,}[\"']"
          "secret.*[\"'][A-Za-z0-9]{20,}[\"']"
          "password.*[\"'][A-Za-z0-9]{8,}[\"']"
          "firebase.*config.*apiKey"
          "sk-ant-[A-Za-z0-9]+"
        )
        
        SECRETS_FOUND=false
        
        for pattern in "${SECRET_PATTERNS[@]}"; do
          if grep -r "$pattern" packages/ frontend/src/ functions/src/ --include="*.ts" --include="*.js" --include="*.tsx" --include="*.jsx" | grep -v "// @ignore-secret" | grep -v "placeholder" | grep -v "mock" | head -1; then
            echo "⚠️ Potential secret found with pattern: $pattern"
            SECRETS_FOUND=true
          fi
        done
        
        if [ "$SECRETS_FOUND" = true ]; then
          echo "❌ Potential secrets detected. Please review and use environment variables."
          echo "Add // @ignore-secret comment for intentional test data."
          exit 1
        else
          echo "✅ No secrets detected"
        fi
        
    - name: License Compliance Check
      run: |
        echo "📋 Checking license compliance..."
        
        # Install license checker
        npm install -g license-checker
        
        # Check licenses in main project
        license-checker --json --out licenses.json
        
        # Check for problematic licenses
        PROBLEMATIC_LICENSES=("GPL" "AGPL" "LGPL")
        
        for license in "${PROBLEMATIC_LICENSES[@]}"; do
          if grep -q "$license" licenses.json; then
            echo "⚠️ Found potentially problematic license: $license"
          fi
        done
        
        echo "✅ License compliance check completed"
        
    - name: Firebase Security Rules Validation
      run: |
        echo "🔥 Validating Firebase security rules..."
        
        if [ -f "firestore.rules" ]; then
          # Install Firebase tools if not available
          npm install -g firebase-tools
          
          # Validate Firestore rules
          firebase firestore:rules:release --test --project demo-cvplus firestore.rules || echo "Security rules validation failed"
        else
          echo "No Firestore rules file found"
        fi
        
        echo "✅ Firebase security validation completed"

  performance-analysis:
    name: Performance Analysis
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout Code
      uses: actions/checkout@v4
      
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        cache-dependency-path: ${{ env.CACHE_DEPENDENCY_PATH }}
        
    - name: Install Dependencies
      run: npm ci
      
    - name: Build Performance Check
      run: |
        echo "⚡ Analyzing build performance..."
        
        START_TIME=$(date +%s)
        
        # Build all packages
        npm run build 2>/dev/null || echo "Build completed with warnings"
        
        END_TIME=$(date +%s)
        BUILD_TIME=$((END_TIME - START_TIME))
        
        echo "Build Performance:"
        echo "- Total build time: ${BUILD_TIME}s"
        
        # Check build time threshold (5 minutes = 300 seconds)
        if [ "$BUILD_TIME" -gt "300" ]; then
          echo "⚠️ Build time (${BUILD_TIME}s) exceeds threshold (300s)"
        else
          echo "✅ Build time within acceptable limits"
        fi
        
        echo "BUILD_TIME=${BUILD_TIME}" >> $GITHUB_ENV
        
    - name: Bundle Size Analysis
      run: |
        echo "📦 Analyzing bundle sizes..."
        
        # Check package sizes
        for pkg in packages/*/dist/; do
          if [ -d "$pkg" ]; then
            SIZE=$(du -sb "$pkg" | cut -f1)
            PKG_NAME=$(basename $(dirname "$pkg"))
            echo "Package $PKG_NAME: $SIZE bytes"
            
            # Set thresholds per package type
            case "$PKG_NAME" in
              "core")
                THRESHOLD=51200  # 50KB
                ;;
              "auth")
                THRESHOLD=102400  # 100KB
                ;;
              "recommendations")
                THRESHOLD=204800  # 200KB
                ;;
              *)
                THRESHOLD=102400  # Default 100KB
                ;;
            esac
            
            if [ "$SIZE" -gt "$THRESHOLD" ]; then
              echo "⚠️ Package $PKG_NAME size ($SIZE bytes) exceeds threshold ($THRESHOLD bytes)"
            else
              echo "✅ Package $PKG_NAME size within limits"
            fi
          fi
        done
        
        # Check frontend bundle if built
        if [ -d "frontend/dist/" ]; then
          FRONTEND_SIZE=$(du -sb frontend/dist/ | cut -f1)
          echo "Frontend bundle: $FRONTEND_SIZE bytes"
          
          # Frontend threshold (5MB)
          if [ "$FRONTEND_SIZE" -gt "5242880" ]; then
            echo "⚠️ Frontend bundle size exceeds 5MB"
          else
            echo "✅ Frontend bundle size within limits"
          fi
        fi
        
    - name: Memory Usage Analysis
      run: |
        echo "🧠 Analyzing memory usage patterns..."
        
        # Simulate package imports and check memory usage
        node -e "
          const initialMemory = process.memoryUsage();
          console.log('Initial memory:', initialMemory.heapUsed, 'bytes');
          
          // Import packages
          try {
            require('./packages/core/dist/index.js');
            require('./packages/auth/dist/index.js');
            require('./packages/recommendations/dist/index.js');
            
            const finalMemory = process.memoryUsage();
            const memoryGrowth = finalMemory.heapUsed - initialMemory.heapUsed;
            
            console.log('Memory growth:', memoryGrowth, 'bytes');
            
            if (memoryGrowth > 52428800) {  // 50MB threshold
              console.log('⚠️ High memory usage detected');
              process.exit(1);
            } else {
              console.log('✅ Memory usage within acceptable limits');
            }
          } catch (error) {
            console.log('Some packages not built, skipping memory test');
          }
        "

  documentation-quality:
    name: Documentation Quality
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout Code
      uses: actions/checkout@v4
      
    - name: Check Documentation Coverage
      run: |
        echo "📚 Checking documentation coverage..."
        
        # Check for README files in packages
        MISSING_DOCS=""
        for pkg in packages/*/; do
          if [ ! -f "${pkg}README.md" ]; then
            MISSING_DOCS="$MISSING_DOCS $(basename $pkg)"
          fi
        done
        
        if [ -n "$MISSING_DOCS" ]; then
          echo "⚠️ Missing README files for packages:$MISSING_DOCS"
        else
          echo "✅ All packages have README files"
        fi
        
        # Check main project documentation
        if [ ! -f "README.md" ]; then
          echo "⚠️ Main README.md is missing"
        else
          echo "✅ Main README.md exists"
        fi
        
    - name: Validate TypeScript Documentation
      run: |
        echo "📝 Checking TypeScript documentation..."
        
        # Check for JSDoc comments in TypeScript files
        UNDOCUMENTED_FUNCTIONS=0
        
        find packages/ -name "*.ts" -not -path "*/node_modules/*" | while read file; do
          # Count exported functions without JSDoc
          FUNCS=$(grep -c "^export function\|^export const.*=" "$file" 2>/dev/null || echo 0)
          DOCS=$(grep -c "/\*\*" "$file" 2>/dev/null || echo 0)
          
          if [ "$FUNCS" -gt "$DOCS" ]; then
            echo "⚠️ $file: $((FUNCS - DOCS)) functions may need documentation"
            UNDOCUMENTED_FUNCTIONS=$((UNDOCUMENTED_FUNCTIONS + FUNCS - DOCS))
          fi
        done
        
        echo "Functions potentially needing documentation: $UNDOCUMENTED_FUNCTIONS"
        
        if [ "$UNDOCUMENTED_FUNCTIONS" -gt "10" ]; then
          echo "⚠️ Many functions may need better documentation"
        else
          echo "✅ Documentation coverage appears adequate"
        fi

  quality-gate-summary:
    name: Quality Gate Summary
    runs-on: ubuntu-latest
    needs: [code-quality, security-analysis, performance-analysis, documentation-quality]
    if: always()
    
    steps:
    - name: Generate Quality Report
      run: |
        echo "# 🛡️ Quality Gates Summary" > quality-summary.md
        echo "" >> quality-summary.md
        echo "**Date**: $(date -u)" >> quality-summary.md
        echo "**Quality Score**: ${{ needs.code-quality.outputs.quality_score }}/100" >> quality-summary.md
        echo "" >> quality-summary.md
        echo "## Gate Results" >> quality-summary.md
        echo "" >> quality-summary.md
        echo "| Quality Gate | Status |" >> quality-summary.md
        echo "|-------------|--------|" >> quality-summary.md
        echo "| Code Quality | ${{ needs.code-quality.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> quality-summary.md
        echo "| Security Analysis | ${{ needs.security-analysis.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> quality-summary.md
        echo "| Performance Analysis | ${{ needs.performance-analysis.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> quality-summary.md
        echo "| Documentation Quality | ${{ needs.documentation-quality.result == 'success' && '✅ Passed' || '❌ Failed' }} |" >> quality-summary.md
        echo "" >> quality-summary.md
        
        # Overall status
        if [ "${{ needs.code-quality.result }}" = "success" ] && \
           [ "${{ needs.security-analysis.result }}" = "success" ] && \
           [ "${{ needs.performance-analysis.result }}" = "success" ] && \
           [ "${{ needs.documentation-quality.result }}" = "success" ]; then
          echo "## ✅ All Quality Gates Passed" >> quality-summary.md
          echo "The code meets all quality standards and is ready for deployment." >> quality-summary.md
        else
          echo "## ❌ Quality Gates Failed" >> quality-summary.md
          echo "Please address the failing quality gates before proceeding." >> quality-summary.md
        fi
        
        echo "" >> quality-summary.md
        echo "## Recommendations" >> quality-summary.md
        echo "- Monitor quality metrics trends" >> quality-summary.md
        echo "- Address any security vulnerabilities promptly" >> quality-summary.md
        echo "- Optimize build performance when possible" >> quality-summary.md
        echo "- Maintain comprehensive documentation" >> quality-summary.md
        
    - name: Upload Quality Summary
      uses: actions/upload-artifact@v4
      with:
        name: quality-gate-summary
        path: quality-summary.md
        
    - name: Update PR Comment (if PR)
      if: github.event_name == 'pull_request'
      run: |
        echo "Quality gate results would be posted as PR comment"
        # In a real implementation, this would post the results to the PR
        
    - name: Update Summary
      run: |
        echo "# 🛡️ Quality Gates Report" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        cat quality-summary.md >> $GITHUB_STEP_SUMMARY